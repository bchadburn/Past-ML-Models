from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import pandas as pd
from xgboost import XGBClassifier
import numpy as np
from nltk.corpus import stopwords
from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix, balanced_accuracy_score
from unidecode import unidecode
import nltk
from sklearn.utils.multiclass import unique_labels
import re

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

class TextSelector(BaseEstimator, TransformerMixin):
    def __init__(self, field):
        self.field = field
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        return X[self.field]

class NumberSelector(BaseEstimator, TransformerMixin):
    def __init__(self, field):
        self.field = field
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        return X[[self.field]]
        
def Tokenizer(str_input):
    words = str_input.lower().split()
    porter_stemmer=nltk.PorterStemmer()
    words = [porter_stemmer.stem(word) for word in words]
    return words    

# Modify stop words
stop_words = ' '.join(list(stop_words))
stop_words = stop_words.replace('i','').replace('ll','')       
replacements = pd.read_csv('job_description_replacements.csv', encoding='latin1') # This is used simply to replace contractions 
replacements.fillna('',inplace=True)
for index, row in replacements.iterrows():
    stop_words = stop_words.replace(row[0],row[1])
    
stop_words = stop_words + 'let everyone given it might did' # add stop words   
stop_words = re.sub(r'[^\w\s]','',stop_words) # remove punctuation  
stop_words = stop_words.strip() # replace extra spaces before and after text

# Read in and format data
data = pd.read_parquet('indeed.parquet', engine = 'fastparquet')

# Remove multidiscipline
data = data[~data['job_title'].str.contains("Generalist/Multidiscipline")]

def clean_titles(df):
    df.loc[: , 'job_title'] = df['job_title'].str.lower()      
    df.loc[: , 'job_title'] = ' ' + df['job_title'] + ' '
    df.loc[: , 'job_title'] = df['job_title'].apply(unidecode)   
                 
    # Title replacements: Loads a dictionary of acronyms, punctuation, and word substitutions
    title_replacements = pd.read_csv('title_replacements.csv',header=None)
    title_replacements.fillna('',inplace=True)
    for index, row in title_replacements.iterrows():
          df.loc[: ,'job_title'] = df['job_title'].str.replace(row[0],row[1])       
                 
    df.loc[: , 'job_title'] = df.loc[: , 'job_title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
    df.loc[: , 'job_title'] = df['job_title'].str.strip() # replace extra spaces before and after text
    return df

def clean_descriptions(df):
    df.loc[: , 'job_description'] = df['job_description'].str.lower()     
    df.loc[: , 'job_description'] = ' ' + df['job_description'] + ' '
    df.loc[: , 'job_description'] = df['job_description'].apply(unidecode)
    
    # Loads skill categories and returns skill categories for each skill found in job description
    skillcluster = pd.read_csv('skillcategories.csv', header=None)
    skillcluster.fillna('',inplace=True)
    for index, row in skillcluster.iterrows():
          df.loc[: ,'job_description'] = df['job_description'].str.replace(row[0],row[1])       

    # job_descriptions: Loads a dictionary of acronyms, punctuation, and word substitutions
    for index, row in replacements.iterrows():
          df.loc[: ,'job_description'] = df['job_description'].str.replace(row[0],row[1])
          
    df.loc[: ,'job_description'] = df.loc[: ,'job_description'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
    df.loc[: ,'job_description'] = df.loc[: ,'job_description'].str.strip() # replace extra spaces before and after text
    return df

clean_titles(data)
clean_descriptions(data)

# Load pre-cleaned parquet file
#data = pd.read_parquet('Z:\Job Mapping\cleaned_indeed.parquet.gzip')

#Some Code if you want to create a classifier to predict function (more general job category such as finance)
#data['JOBSPECIALTYFUNCTION'] = data['JOBSPECIALTYCODE'].str[:3]
#data['pred_function'] = classifier_function.predict(data[['job_title','job_description']])
#X = data[['job_title', 'job_description', 'pred_function']]

X = data[['job_title', 'job_description']]
Y = data['JOBSPECIALTYCODE']

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 99)

classifier = Pipeline([
    ('features', FeatureUnion([
        ('text_jd', Pipeline([
            ('coljd', TextSelector('job_title')),
            ('tfidfjd', TfidfVectorizer(tokenizer=Tokenizer, 
                     min_df=.0025, max_df=0.25, ngram_range=(1), max_features = 8000)),
        ])),
        ('text_title', Pipeline([
            ('coltitle', TextSelector('job_description')),
            ('tfidftitle', TfidfVectorizer(tokenizer=Tokenizer, 
                     min_df=.0025, max_df=0.25, ngram_range=(1,2), max_features = 8000)),
        ])),
    ])),
    ('clf', XGBClassifier(objective='multi:softmax',njobs =-1, nrounds = 5, gamma = 0, 
                          max_depth = 6, min_child_weight = 6, learning_rate = .2, alpha =1, clf__scale_pos_weight = 1,
                          clf__max_df = ('l1','l2'), seed = 1)),
    ])

classifier.fit(x_train, y_train)
preds = classifier.predict(x_test)

# Review Prediction Metrics
print("Accuracy:", accuracy_score(y_test, preds))
print(balanced_accuracy_score(y_test, preds))
print(f1_score(y_test, preds, average = 'macro'))

# Check for overfitting in training set
#preds_train = classifier.predict(x_train) 
#print("Accuracy:", accuracy_score(y_train, preds_train))
#print(classification_report(y_train, preds_train))
#print(confusion_matrix(y_train, preds_train))
#print(balanced_accuracy_score(y_train, preds_train))
#print("Precision:", precision_score(y_test, preds))

classreport = classification_report(y_test, preds)
confmtx_labels = np.array(unique_labels(y_test, preds))
confmtx = confusion_matrix(y_test, preds)
                                
# Review predictions vs actuals
review = pd.DataFrame({'Prediction': preds, 'Actual' : y_test, 'Title': x_test.job_title, 'Description' : x_test.job_description})
review['Incorrect'] = ''

for index, row in review.iterrows():
    if row['Prediction'] == row['Actual']:
        row['Incorrect'] = 0
    else: 
        row['Incorrect'] = 1

# Export predictions vs actuals        
pred_actuals = pd.merge(review, jobspecialtycodes, left_on=['Prediction'], right_on=['2018 Code'], how='left')
pred_actuals.rename(columns = {'2018 Code': 'Prediction Code', '2018 Title': 'Prediction Title', '2018 Description': 'Prediction Description'}, inplace = True)
pred_actuals.to_csv('predicted_vs_actuals_clusters.csv')
