Summary
This project was used to create a taxonomy of job titles related to "data scientist" and to determine potential equivalent classes,
similar titles and sub-domains. The results indicated four titles including data scientist, data analyst, data engineer, and
machine learning engineer. ML engineer was clearly the most distinct category. While data engineer was the most fragmented between 
other classes. Analyst, was also fairly distinguishable although the Data Scientist title indicated similarity with analyst and 
machine learning. It also appears that big data is also somewhat distinguishable from other analyst roles. 

Files
1. indeed_webscrapping provides the python code for scrapping indeed using beautiful soup without access to the API. API access could be
simpler for querying titles but requires approval.
2. jobopenings.json provides around 1,000 job openings scrapped from indeed with the titles mentioned previously. 
3. indeed_doc_classification.ipynb. Provides the NLP model code including TF-IDF, Doc2Vec, NMF, LDA and topic visualizations. TF-IDF 
worked well for this task, although both NMF and LDA also provided useful insight.

Full Description

Executive Summary
The objective of this project was to begin to create a taxonomy of job titles including an investigation in potentially synonymous or 
closely related labels. Initially a search was conducted to extract the most common terms for a given job title. TF-IDF was used to 
identify useful bi-grams and tr-grams that were then used for locating a longer list of related job titles. The process was then 
re-iterated using the list of most popular job titles to create a corpus for each title. This approach is an improvement to the earlier 
methods I used in this course as the titles are required to be found in the job title on Indeed. We also have a much larger corpus also 
including job openings in the U.S. and U.K. 

Multidimensional scaling was conducted using the vector matrices using t-distributed stochastic neighbor embedding (t-SNE). TF-IDF and 
analyst judgement (applying equivalent classes and still using TF-IDF) both performed better than neural network embeddings in terms of 
grouping the most common job titles. K-means, T-SNE, and even PCA were also used to evaluate groupings by topics while also comparing 
the use of bi-grams and unigrams for evaluating term usage. Hierarchical clustering was also used to further investigate the 
relationships between job titles. 

Latent Dirichlet Allocation (LDA) was  conducted to create a probabilistic model for topic modeling. Grid search was used for 
parameter tuning which included evaluating different number of topics by log likelihood scores to determine the best fit for the 
LDA model. Using k-means, T-SNE, LDA, and PCA, it became clear that the topics were most clearly delineated by two distinct clusters 
one primarily related to data analyst type skills and terms and another for machine learning terms.  Literature Review
TF-IDF and word embeddings techniques have been used for a variety of natural language processing models and have been effective 
approaches for classifying documents, sentiment analysis, and evaluating document meaning. In Deep Learning for NLP, Brownlee 
introduces simple implementations of both approaches which were used as templates in my code1. Brownlee also provides examples of 
cleaning data before modeling begins which was used although greatly modified in my code in order to add further cleaning and stemming 
of the data.

I built on skeleton code provided by Paul Hyunh, the TA in my 453 natural language processing class. I also referred to similar work 
posted online by Jeff Delaney and W. Shaui(2,3).  Scikit-learn documentation was also used to further explore the data using 
biclustering methods using the Spectral Co-clustering algorithm(4). I also referred to some material on stack overflow on how to 
conduct grid search(5). I also referred to Izenman’s discussion on clustering analysis and hierarchical clustering. While he doesn’t 
provide the python code, he dives into the math behind the algorithms and the different techniques along with several examples of 
computing cluster distances(6). Finally, for Latent Dirichlet Allocation I found a very thorough review of applying LDA as well as an 
example for using pyLDAvis package(7). 

MethodsTo start the process of creating a taxonomy of job titles, currently known job titles were searched to identify common key 
words used in postings. These key words were then used to conduct new searches to identify related job titles. The process was then 
reiterated with the new job titles to create a larger corpus of job postings. Four major job titles were identified from the corpus 
including data scientist, data engineer, machine learning and data analyst. The titles were standardized using these four titles, 
although titles without any of these sets of titles were assigned as ‘other’. 
Three vector matrices were created using tf-idf and word embeddings to compare performance between the three approaches. K-means 
clustering was used with different sized clusters to determine the optimal number of clusters for separating the job openings. 
For each cluster, a list of example documents as well as the terms was also provided in order to review the type of words common 
for each cluster. Multidimensional scaling was conducted using t-distributed stochastic neighbor embedding (t-SNE) in order to 
project dimensions to 2d view. A PCA model was also provided to view clusters. Performance was also compared using bi-grams compared 
to unigram when creating the TF-IDF matrix. 

Multiple Hierarchical cluster analysis was performed to help assist with understanding the relationships between topics. We also used 
Non-negative Matrix Factorization (NMF) as well as Latent Dirichlet Allocation (LDA) to evaluate differences in topics. Latent 
Dirichlet Allocation (LDA) creates a probabilistic model of how documents are grouped by latent topics. In other words, we evaluate 
common topics by usage of related words and determine which documents are most related to those identified topics. Grid search was 
used for parameter tuning which includes evaluating different number of topics by log likelihood scores to determine the best fit 
for the LDA model. The package pyLDAavis was also used to visualize the clusters along with the relevance and frequency of terms 
for each. 

Results
As previously seen, the four most common job titles were analyst, data engineer, data scientist, and machine learning engineer. 
TFIDF provides better insight into the differences or nature of relationships between data science related job titles than neural 
network embeddings. We can see there is a lot of overlap between the identified jobs. However, the most distinct categories are 
clearly machine learning and all other job positions. This is true whether we are using unigrams, bigrams, k-means, T-SNE or 
Hierarchical cluster analysis. 

This time we also saw a much closer performance between bi-grams and unigrams using k-means. In fact, bi-grams were somewhat more 
useful for clustering and with our new data we even see three distinct clusters between analyst, data scientist and machine learning 
engineer (see figure 2). PCA wasn’t overly helpful for plotting clusters, although it seemed to perform well for unigrams with two 
groups. Bi-blustering was also not very insightful as almost all documents ended up being grouped together. Non-negative Matrix 
Factorization (NMF) on the other hand actually seemed to have better insights initially than LDA. As shown in figure 6, LDA provides 
popular terms for four topics, but only two seem interpretable (topic 1 & 3: similar to analyst vs machine learning). While NMF seems 
to have a cluster for analyst (topic 3), machine learning (topic 1), and what seems to be words associated with big data (topic 2). 
Although topic 0 seems meaningless, NMF seems to provide some additional insight not identified by LDA and warrants further 
exploration.

Grid search was then conducted to help tune LDA parameters. A plot was provided to show the log likelihood change by number of 
components and learning_decay rate (see figure 7). This seems to indicate two clusters seems to perform the best, as previously 
seen, and falls steadily from there. I was hoping to see increased performance at four clusters (the number of job titles) but we 
don’t see that. Instead, when we use pyLDAvis to plot the clusters with word frequency, the two clusters are easily interpretable 
(see figure 8) and when using four clusters, cluster 3 and 4, don’t seem very interpretable and have little to do with our previously 
identified job titles.

Conclusion
This method provides a good approach for identifying related job titles and identifying titles that can be considered distinct titles. However, it is limited in its ability to provide a fully outlined ontology of how the terms are related or fully vetting equivalent classes. There also is no one size fits all approach as some-times bi-grams may work better and for another unigrams or even trigrams. It also seems NMF may work better than LDA, so there are several decisions that will need to be made by the analyst’s judgement.
In our current application of evaluating ‘data science’ related roles, machine learning was found to be the most distinct job title which was an unexpected finding. K-means also seemed to provide useful insight into some of the cross over between data scientist vs analyst as the fragmented nature of a data engineer title which seems to entail a large amount of skills used in the data science and analyst roles. Overall, I found LDA and NMF were also insightful for connecting clusters with words and pyLDAvis for plotting and reviewing topics. More work could be conducted to further delve into the NMF clusters to determine how well it separated three clusters (as it seemed to also be detecting ‘big data’ openings) and what titles they were associated with. It might also help to incorporate pay ranges to assist with topic modeling which may also include evaluating junior vs senior level positions. I was also disappointed by doc2vec’s performance and would like to look at other options such as BERT or LSTM models to compare performance. 

References

1Brownlee, J. (2019). Deep Learning for Natural Language Processing: Develop Deep Learning Models for Natural Language in Python.
2Delaney, J. (2017). Visualizing Word Vectors with t-SNE. Retrieved from https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne.
3W, Shaui. (2018, Dec 22). Topic Modeling and t-SNE Visualization. Retrieved from https://shuaiw.github.io/2016/12/22/topic-modeling-and-tsne-visualzation.html
4Scikitlearn.org [2016]. Biclustering documents with the Spectral Co-clustering algorithm. Retrieved  from https://scikit-learn.org/0.18/auto_examples/bicluster/bicluster_newsgroups.html
5stackoverflow.com [2017]. How to graph grid scores from GridSearchCV? Retrieved from
 https://stackoverflow.com/questions/37161563/how-to-graph-grid-scores-from-gridsearchcv
6Izenman, A. (2008). Cluster Analysis. Modern Multivariate Statistical Techniques (pp. 407-462). DOI 10.1007/978-0-387-78189-1.
7machinelearningplus.com [2018]. LDA in Python – How to grid search best topic models? https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/ 

